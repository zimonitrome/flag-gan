{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import Generator, Discriminator\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"try13\"\n",
    "data = \"../data/training_data\"\n",
    "batch_size = 300\n",
    "lr = 0.0005\n",
    "betas = (0.5, 0.99)\n",
    "weight_decay = 0.00001\n",
    "epoch_test_interval = 1\n",
    "batch_test_interval = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run on cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print(f\"Will run on {device}.\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "noise = Variable(torch.randn(1, 100, 1, 1)).to(device)\n",
    "test_noise = Variable(torch.randn(9, 100, 1, 1)).to(device)\n",
    "\n",
    "document_path = f\"../data/documentation/{test_name}\"\n",
    "progress_path = f\"{document_path}/gan_progress\"\n",
    "os.makedirs(progress_path, exist_ok=True)\n",
    "\n",
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGBA')\n",
    "\n",
    "\n",
    "def get_flag_loader(dir=data, batch_size=batch_size, shuffle=True):\n",
    "    transform = transforms.ToTensor()\n",
    "    flag_dataset = datasets.ImageFolder(\n",
    "        root=dir, transform=transform, loader=pil_loader)\n",
    "    flag_loader = torch.utils.data.DataLoader(\n",
    "        dataset=flag_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return flag_loader\n",
    "\n",
    "\n",
    "def train(data_loader, epoch):\n",
    "    D.train()\n",
    "    G.train()\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        n_samples = data.size(dim=0)\n",
    "\n",
    "        #######################\n",
    "        # Train Discriminator #\n",
    "        #######################\n",
    "\n",
    "        # Zero out gradients on discriminator\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Load real flag data, run through discriminator and compute BCE loss\n",
    "        # against target vector of all ones, because the flags are legit\n",
    "        real_data = Variable(data).to(device)\n",
    "        output = D(real_data)\n",
    "        real_target = Variable(torch.ones(n_samples)).to(device)\n",
    "        real_error = loss(output.squeeze(), real_target)\n",
    "\n",
    "        # Get normally distributed noise and feed to generator to create fake\n",
    "        # flag data. Run fake flag data through discriminator and compute BCE\n",
    "        # loss against target vector of all zeros, because data is fake. Detach\n",
    "        # to avoid training generator on these labels\n",
    "        noise = Variable(torch.randn(n_samples, 100, 1, 1)).to(device)\n",
    "        fake_data = G(noise)\n",
    "        output = D(fake_data.detach()).to(device)\n",
    "        fake_target = Variable(torch.zeros(n_samples)).to(device)\n",
    "        fake_error = loss(output.squeeze(), fake_target)\n",
    "\n",
    "        # Compute accumulated gradient based on real and fake data to update\n",
    "        # discriminator weights\n",
    "        d_error = real_error + fake_error\n",
    "        d_error.backward()\n",
    "        d_optim.step()\n",
    "\n",
    "        ###################\n",
    "        # Train Generator #\n",
    "        ###################\n",
    "\n",
    "        # Zero out gradients on generator\n",
    "        G.zero_grad()\n",
    "\n",
    "        # Run fake flag data through discriminator and compute BCE loss against\n",
    "        # target vector of all ones. We want to fool the discriminator, so\n",
    "        # pretend the mapped data is genuine\n",
    "        output = D(fake_data)\n",
    "        g_error = loss(output.squeeze(), real_target)\n",
    "\n",
    "        # Compute new gradients from discriminator and update weights of the generator\n",
    "        g_error.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        if epoch % epoch_test_interval == 0 and batch_idx % batch_test_interval == 0:\n",
    "            # Logging\n",
    "            log_text = '({:02d}, {:02d}) \\tLoss_D: {:.6f} \\tLoss_G: {:.6f}'.format(epoch, batch_idx, d_error.data, g_error.data)\n",
    "            print(log_text)\n",
    "            file = open(f\"{document_path}/loss.txt\", \"a+\")\n",
    "            file.write(log_text + \"\\n\")\n",
    "            \n",
    "            # Test Generator\n",
    "            with torch.no_grad():\n",
    "                sample = G(test_noise).detach().cpu()\n",
    "            grid = vutils.make_grid(sample, padding=2, normalize=True, nrow=3)\n",
    "            img = np.transpose(grid, (1,2,0)).numpy()\n",
    "            plt.clf()\n",
    "            imgplot = plt.imshow(img)\n",
    "            plt.title(f\"Epoch {epoch}\")\n",
    "            plt.savefig(f\"{progress_path}/{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, D = Generator().to(device), Discriminator().to(device)\n",
    "loss = nn.BCELoss()\n",
    "g_optim = optim.AdamW(G.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "d_optim = optim.AdamW(D.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "\n",
    "flag_loader = get_flag_loader()\n",
    "noise = Variable(torch.randn(1, 100, 1, 1))\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Load previous network progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_file_name = \"../data/documentation/try16/net_D_e748.pth\"\n",
    "G_file_name = \"../data/documentation/try16/net_G_e748.pth\"\n",
    "\n",
    "D.load_state_dict(torch.load(D_file_name))\n",
    "G.load_state_dict(torch.load(G_file_name))\n",
    "\n",
    "epoch = int(re.findall(r\"net\\_G\\_e(\\d+)\", G_file_name)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 00) \tLoss_D: 0.008919 \tLoss_G: 14.757826\n",
      "(549, 00) \tLoss_D: 0.000177 \tLoss_G: 10.585363\n",
      "(550, 00) \tLoss_D: 0.002067 \tLoss_G: 12.778118\n",
      "(551, 00) \tLoss_D: 0.000024 \tLoss_G: 11.688669\n",
      "(552, 00) \tLoss_D: 0.000042 \tLoss_G: 11.379169\n",
      "(553, 00) \tLoss_D: 0.000637 \tLoss_G: 24.427641\n",
      "(554, 00) \tLoss_D: 0.000714 \tLoss_G: 9.207132\n",
      "(555, 00) \tLoss_D: 0.000682 \tLoss_G: 9.294248\n",
      "(556, 00) \tLoss_D: 0.002192 \tLoss_G: 10.770787\n",
      "(557, 00) \tLoss_D: 0.000604 \tLoss_G: 20.357693\n",
      "(558, 00) \tLoss_D: 0.000107 \tLoss_G: 13.682560\n",
      "(559, 00) \tLoss_D: 0.000406 \tLoss_G: 8.906823\n",
      "(560, 00) \tLoss_D: 0.001476 \tLoss_G: 8.075767\n",
      "(561, 00) \tLoss_D: 0.000347 \tLoss_G: 8.846148\n",
      "(562, 00) \tLoss_D: 0.001371 \tLoss_G: 8.479985\n",
      "(563, 00) \tLoss_D: 0.000656 \tLoss_G: 8.389080\n",
      "(564, 00) \tLoss_D: 0.000301 \tLoss_G: 9.084194\n",
      "(565, 00) \tLoss_D: 0.000030 \tLoss_G: 21.877491\n",
      "(566, 00) \tLoss_D: 0.000072 \tLoss_G: 14.355715\n",
      "(567, 00) \tLoss_D: 0.000074 \tLoss_G: 11.325435\n",
      "(568, 00) \tLoss_D: 0.000064 \tLoss_G: 13.153550\n",
      "(569, 00) \tLoss_D: 0.001231 \tLoss_G: 11.335192\n",
      "(570, 00) \tLoss_D: 0.005086 \tLoss_G: 12.973780\n",
      "(571, 00) \tLoss_D: 0.000237 \tLoss_G: 10.432567\n",
      "(572, 00) \tLoss_D: 0.001013 \tLoss_G: 9.172129\n",
      "(573, 00) \tLoss_D: 0.000075 \tLoss_G: 13.377459\n",
      "(574, 00) \tLoss_D: 0.008083 \tLoss_G: 18.864084\n",
      "(575, 00) \tLoss_D: 0.000021 \tLoss_G: 20.985687\n",
      "(576, 00) \tLoss_D: 0.000039 \tLoss_G: 17.795536\n",
      "(577, 00) \tLoss_D: 0.000158 \tLoss_G: 15.162962\n",
      "(578, 00) \tLoss_D: 0.000072 \tLoss_G: 13.117278\n",
      "(579, 00) \tLoss_D: 0.002867 \tLoss_G: 14.171263\n",
      "(580, 00) \tLoss_D: 0.000426 \tLoss_G: 10.570354\n",
      "(581, 00) \tLoss_D: 0.000094 \tLoss_G: 11.247848\n",
      "(582, 00) \tLoss_D: 0.000037 \tLoss_G: 11.613669\n",
      "(583, 00) \tLoss_D: 0.000216 \tLoss_G: 10.927555\n",
      "(584, 00) \tLoss_D: 0.000263 \tLoss_G: 12.702021\n",
      "(585, 00) \tLoss_D: 0.000020 \tLoss_G: 13.185994\n",
      "(586, 00) \tLoss_D: 0.000043 \tLoss_G: 12.405773\n",
      "(587, 00) \tLoss_D: 0.000045 \tLoss_G: 12.449095\n",
      "(588, 00) \tLoss_D: 0.000034 \tLoss_G: 11.896410\n",
      "(589, 00) \tLoss_D: 0.000014 \tLoss_G: 15.037526\n",
      "(590, 00) \tLoss_D: 0.000015 \tLoss_G: 12.952058\n",
      "(591, 00) \tLoss_D: 0.000028 \tLoss_G: 15.693633\n",
      "(592, 00) \tLoss_D: 0.000147 \tLoss_G: 12.132936\n",
      "(593, 00) \tLoss_D: 0.000041 \tLoss_G: 11.767638\n",
      "(594, 00) \tLoss_D: 0.000063 \tLoss_G: 11.560111\n",
      "(595, 00) \tLoss_D: 0.000025 \tLoss_G: 11.765694\n",
      "(596, 00) \tLoss_D: 0.000015 \tLoss_G: 12.536755\n",
      "(597, 00) \tLoss_D: 0.000054 \tLoss_G: 11.442035\n",
      "(598, 00) \tLoss_D: 0.000037 \tLoss_G: 13.907029\n",
      "(599, 00) \tLoss_D: 0.000009 \tLoss_G: 13.203717\n",
      "(600, 00) \tLoss_D: 0.000013 \tLoss_G: 13.120164\n",
      "(601, 00) \tLoss_D: 0.000017 \tLoss_G: 13.163127\n",
      "(602, 00) \tLoss_D: 0.000027 \tLoss_G: 10.753302\n",
      "(603, 00) \tLoss_D: 0.000004 \tLoss_G: 13.419210\n",
      "(604, 00) \tLoss_D: 0.000004 \tLoss_G: 14.477076\n",
      "(605, 00) \tLoss_D: 0.000011 \tLoss_G: 13.679785\n",
      "(606, 00) \tLoss_D: 0.000001 \tLoss_G: 17.782520\n",
      "(607, 00) \tLoss_D: 0.000008 \tLoss_G: 12.462325\n",
      "(608, 00) \tLoss_D: 0.000004 \tLoss_G: 13.739504\n",
      "(609, 00) \tLoss_D: 0.000005 \tLoss_G: 14.060378\n",
      "(610, 00) \tLoss_D: 0.000015 \tLoss_G: 12.108742\n",
      "(611, 00) \tLoss_D: 0.000004 \tLoss_G: 14.704684\n",
      "(612, 00) \tLoss_D: 0.000004 \tLoss_G: 13.494608\n",
      "(613, 00) \tLoss_D: 0.000003 \tLoss_G: 13.634660\n",
      "(614, 00) \tLoss_D: 0.000002 \tLoss_G: 14.436810\n",
      "(615, 00) \tLoss_D: 0.000002 \tLoss_G: 14.850535\n",
      "(616, 00) \tLoss_D: 0.000005 \tLoss_G: 13.251779\n",
      "(617, 00) \tLoss_D: 0.000004 \tLoss_G: 13.397658\n",
      "(618, 00) \tLoss_D: 0.000001 \tLoss_G: 14.611137\n",
      "(619, 00) \tLoss_D: 0.000002 \tLoss_G: 13.834362\n",
      "(620, 00) \tLoss_D: 0.000001 \tLoss_G: 15.021530\n",
      "(621, 00) \tLoss_D: 0.000000 \tLoss_G: 16.168009\n",
      "(622, 00) \tLoss_D: 0.000001 \tLoss_G: 15.473052\n",
      "(623, 00) \tLoss_D: 0.000000 \tLoss_G: 16.667685\n",
      "(624, 00) \tLoss_D: 0.000000 \tLoss_G: 16.252663\n",
      "(625, 00) \tLoss_D: 0.000000 \tLoss_G: 16.805326\n",
      "(626, 00) \tLoss_D: 0.000002 \tLoss_G: 15.592930\n",
      "(627, 00) \tLoss_D: 0.000001 \tLoss_G: 14.952712\n",
      "(628, 00) \tLoss_D: 0.000000 \tLoss_G: 16.801828\n",
      "(629, 00) \tLoss_D: 0.000000 \tLoss_G: 17.544367\n",
      "(630, 00) \tLoss_D: 0.000000 \tLoss_G: 16.722454\n",
      "(631, 00) \tLoss_D: 0.000002 \tLoss_G: 15.029122\n",
      "(632, 00) \tLoss_D: 0.000000 \tLoss_G: 16.656116\n",
      "(633, 00) \tLoss_D: 0.000000 \tLoss_G: 17.545401\n",
      "(634, 00) \tLoss_D: 0.000000 \tLoss_G: 17.050688\n",
      "(635, 00) \tLoss_D: 0.000000 \tLoss_G: 21.779684\n",
      "(636, 00) \tLoss_D: 0.000000 \tLoss_G: 17.172773\n",
      "(637, 00) \tLoss_D: 0.000001 \tLoss_G: 17.499334\n",
      "(638, 00) \tLoss_D: 0.000001 \tLoss_G: 17.583925\n",
      "(639, 00) \tLoss_D: 0.000001 \tLoss_G: 15.556855\n",
      "(640, 00) \tLoss_D: 0.000000 \tLoss_G: 19.440750\n",
      "(641, 00) \tLoss_D: 0.000000 \tLoss_G: 19.282440\n",
      "(642, 00) \tLoss_D: 0.000000 \tLoss_G: 17.709198\n",
      "(643, 00) \tLoss_D: 0.000000 \tLoss_G: 17.652647\n",
      "(644, 00) \tLoss_D: 0.000000 \tLoss_G: 19.180826\n",
      "(645, 00) \tLoss_D: 0.000000 \tLoss_G: 19.218439\n",
      "(646, 00) \tLoss_D: 0.000000 \tLoss_G: 17.079912\n",
      "(647, 00) \tLoss_D: 0.000000 \tLoss_G: 17.083040\n",
      "(648, 00) \tLoss_D: 0.000005 \tLoss_G: 36.123116\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    train(data_loader=flag_loader, epoch=epoch)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Save networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(), f\"{document_path}/net_G_e{epoch}.pth\")\n",
    "torch.save(D.state_dict(), f\"{document_path}/net_D_e{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
